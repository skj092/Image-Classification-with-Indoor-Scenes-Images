{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom glob import glob\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport torchvision\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:06:17.529334Z","iopub.execute_input":"2022-02-05T09:06:17.529635Z","iopub.status.idle":"2022-02-05T09:06:17.537694Z","shell.execute_reply.started":"2022-02-05T09:06:17.529604Z","shell.execute_reply":"2022-02-05T09:06:17.536438Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"!pip install pretrainedmodels","metadata":{"execution":{"iopub.status.busy":"2022-02-05T08:42:18.840938Z","iopub.execute_input":"2022-02-05T08:42:18.841236Z","iopub.status.idle":"2022-02-05T08:42:32.806659Z","shell.execute_reply.started":"2022-02-05T08:42:18.841191Z","shell.execute_reply":"2022-02-05T08:42:32.805343Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pretrainedmodels\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n     |████████████████████████████████| 58 kB 556 kB/s            \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (1.9.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (0.10.1)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (4.62.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels) (1.16.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (3.10.0.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (1.20.3)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (8.2.0)\nBuilding wheels for collected packages: pretrainedmodels\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=41465a37ba631f227111ff8e333b27d362c14ef27c5a1b94979403df5265e20a\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built pretrainedmodels\nInstalling collected packages: pretrainedmodels\nSuccessfully installed pretrainedmodels-0.7.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"class IndoorData:\n    def __init__(self, images, transform=None):\n        self.images = images\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        img = Image.open(img_path).convert('RGB')\n        label = img_path.parent.name   \n        label = torch.tensor(lbl2idx[label])\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:06:19.906777Z","iopub.execute_input":"2022-02-05T09:06:19.907090Z","iopub.status.idle":"2022-02-05T09:06:19.914590Z","shell.execute_reply.started":"2022-02-05T09:06:19.907049Z","shell.execute_reply":"2022-02-05T09:06:19.913435Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import pretrainedmodels\nimport torch.nn as nn\n\ndef Model(pretrained):\n    if pretrained:\n        model = pretrainedmodels.__dict__['resnet18'](\n            pretrained='imagenet')\n    else:\n        model = pretrainedmodels.__dict__['resnet18'](\n            pretrained=None)\n    model.last_linear = nn.Sequential(\n        nn.Linear(in_features=512, out_features=67),\n        nn.Sigmoid())\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:06:20.384715Z","iopub.execute_input":"2022-02-05T09:06:20.385154Z","iopub.status.idle":"2022-02-05T09:06:20.392888Z","shell.execute_reply.started":"2022-02-05T09:06:20.385119Z","shell.execute_reply":"2022-02-05T09:06:20.391848Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn as nn\ncriterion = nn.CrossEntropyLoss()\n\ndef train(dataloader, model, optimizer, device):\n    model.train()\n    tr_loss = 0\n    tk0 = tqdm(train_dl, desc='Train')\n    for step, batch in enumerate(tk0):\n        inputs = batch[0]\n        target = batch[1]\n\n        inputs = inputs.to(device, dtype=torch.float)\n        targets = target.to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, targets)\n        loss.backward()\n        tr_loss += loss\n        optimizer.step()\n    \ndef evaluate(dataloader, model, device):\n    model.eval()\n    val_loss = 0\n    val_preds = None\n    val_labels = None\n    tk0 = tqdm(dataloader, desc='Validation')\n    \n    for step, batch in enumerate(tk0):\n        inputs = batch[0]\n        targets = batch[1]\n        \n        inputs = inputs.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.long)\n        \n        output = model(inputs)\n        loss = criterion(output, targets)\n        \n        val_loss+= loss.item()\n        \n    return val_loss","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:06:23.599076Z","iopub.execute_input":"2022-02-05T09:06:23.599507Z","iopub.status.idle":"2022-02-05T09:06:23.611275Z","shell.execute_reply.started":"2022-02-05T09:06:23.599470Z","shell.execute_reply":"2022-02-05T09:06:23.610278Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"bs = 64\nepochs=3\n\np = Path('.')\nimages = list(p.glob(\"../input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/**/*.jpg\"))\n\nlabels = [img.parent.name for img in images]\nlabels = set(labels)\nlbl2idx = {lbl: idx for idx, lbl in enumerate(labels)}\n\ntrain_transform = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n        transforms.ToTensor(),\n    ]\n)\n\nsplit = int(len(images)*0.8)\ntrain_ds = IndoorData(images[:split], train_transform)\nvalid_ds = IndoorData(images[split:], train_transform)\n\ntrain_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=54, shuffle=False)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = Model(pretrained=True)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\nplist = [{'params': model.parameters(), 'lr': 1e-3}]\noptimizer = torch.optim.Adam(plist, lr=5e-5)\n\nfor epoch in range(epochs):\n    train(train_dl, model, optimizer, device)\n    val_loss = evaluate(valid_dl, model, device)\n    print(f'epoch = {epoch}, valid loss = {val_loss}')\ntorch.save(model.state_dict(), 'model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:22:59.633324Z","iopub.execute_input":"2022-02-05T09:22:59.633662Z","iopub.status.idle":"2022-02-05T09:22:59.731595Z","shell.execute_reply.started":"2022-02-05T09:22:59.633629Z","shell.execute_reply":"2022-02-05T09:22:59.730503Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}